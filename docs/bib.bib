
@Comment concept of intentional/unintentional attacks. Trains an ngram model + SVM classifier on clean/stegged text. Attacked tlex using its lack of context awareness. 84.9% accuracy on detecting stegged text.
@inproceedings{lexsteg,
  title={Attacks on lexical natural language steganography systems},
  author={Taskiran, Cuneyt M and Topkara, Umut and Topkara, Mercan and Delp, Edward J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VIII},
  volume={6072},
  pages={607209},
  year={2006},
  organization={International Society for Optics and Photonics}
}


@Comment tyrannosaurus lex. http://web.mit.edu/keithw/tlex/ synonym based steg. Has perl implementation. Watermarking of playboy articles. Suggests WordNet to create synonym tables. Naive method suffers from one-way synonyms i.e. too -> also "the bed was too big". Only 30% of words are deemed 'useful'. Does not take context into consideration.
@inproceedings{tlex,
  title={Lexical steganography through adaptive modulation of the word choice hash},
  author={Winstein, K},
  booktitle={Secondary education at the Illinois Mathematics and Science Academy},
  year={1999},
}

@Comment uses ascii value of message to select synonym for replacement. Seeems to confuse the meaning of 'cover text'
@inproceedings{lunabel,
  title={Exploiting linguistic features in lexical steganography: design and proof-of-concept implementation},
  author={Chand, Vineeta and Orgun, C Orhan},
  booktitle={System Sciences, 2006. HICSS'06. Proceedings of the 39th Annual Hawaii International Conference on},
  volume={6},
  pages={126b--126b},
  year={2006},
  organization={IEEE}
}

@Comment Encodes using word ordering. Honestly seems a bit rubbish.
@article{chang2012secret,
  title={The secret’s in the word order: Text-to-text generation for linguistic steganography},
  author={Chang, Ching-Yun and Clark, Stephen},
  journal={Proceedings of COLING 2012},
  pages={511--528},
  year={2012}
}

@Comment uses enron email + twitter datasets. Maps bit blocks to sets of words. e.g. 00 -> {big, potato, orange} etc. Weakness in that capacity is constant throughout encoding. E.g. "the big large dog ate food" HAS to store as much data as "3-Sat is an NP-Hard decision problem". Trained their own word embeddings for some reason. Forces user to use whatever cover text it generates - often pretty rubbish. Does not use a GAN.
@article{lstm,
  title={Generating Steganographic Text with LSTMs},
  author={Fang, Tina and Jaggi, Martin and Argyraki, Katerina},
  journal={arXiv preprint arXiv:1705.10742},
  year={2017}
}

@Comment States that encoded vector is a bottleneck. Instead encoder outputs a sequence of vectors and decoder chooses which vectors to use. Potential use for lexsteg in determining words that have a large capacity? Potentially encode word to sequence of 'potential embedding vectors', then decode with message to original input + message? Need to consider gap between output word2vec and actual words. i.e. need to include gradient of vec -> closest word or training will be dodgy. Can we use the same annotation weight forumula? uses a bi-directional rnn to get hidden states in both directions. Look up beam search. 
@article{seq2seq,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{seq2seq2,
  title={Multi-task sequence to sequence learning},
  author={Luong, Minh-Thang and Le, Quoc V and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
  journal={arXiv preprint arXiv:1511.06114},
  year={2015}
}

@Comment Yelp corpus. Really interesting idea but not sure why they give the discriminator the hidden state instead of the output, english is quite poor and only discussion of accuracy instead of f1
@article{advseq2seq,
  title={Sentiment Transfer using Seq2Seq Adversarial Autoencoders},
  author={Singh, Ayush and Palod, Ritu},
  journal={arXiv preprint arXiv:1804.04003},
  year={2018}
}

@Comment Focus on image steg with JPEG. Traditional methods relying on hiding in DCT coeffs can be discovered by checking histograms. New methods of steganalysis use supervised learning. Hides data in random positions, needs error correction to compensate. Introduces idea of ε-security. Discusses high embedding strength i.e. embedding across the entire image so that cover image statistics cannot be derived vs randomised hiding.
@inproceedings{yass,
  title={YASS: Yet another steganographic scheme that resists blind steganalysis},
  author={Solanki, Kaushal and Sarkar, Anindya and Manjunath, BS},
  booktitle={International Workshop on Information Hiding},
  pages={16--31},
  year={2007},
  organization={Springer}
}

@Comment seems to just generate cover images and then apply a standard stego algo
@article{gan,
  title={Steganographic generative adversarial networks},
  author={Volkhonskiy, Denis and Nazarov, Ivan and Borisenko, Boris and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1703.05502},
  year={2017}
}

@Comment who couldve guessed that the only paper that gives real information is from Oxford? Argues that steganalysis attacks on cover generation are pointless as the generation systems can be defeated by just looking at them. Most papers have litte mention of the security provided and instead look at just syntactic/semantic properties. Performed their own steganalysis and showed that individual tweet = good but a couple of samples and it becomes easy to detect. Defines 'source coding' LOOK AT J. Fridrich, Steganography in digital media: principles, algorithms, and applications. Cambridge University Press, 2009. as something that solves the 'selection channel problem' i.e. some words have better capacity. Uses PPDB for transformations. Might be useful. Really like their user interface -> user gives cover text, generates many potential stegotexts and then user selects best. Could even train network on user input to give better stegotexts over time. Capacity of stegosystem is log2 of number of paraphrases/synonyms. May want to start off by analysing capacity of our datasets with respect to word2vec synonyms. Looks at measuring distortion with respect to binary, proabilistic, edit distance and feature vectors. Dataset of 21M tweets!!!! Square root law of steganography. 
@article{covertweet,
  title={Avoiding detection on twitter: embedding strategies for linguistic steganography},
  author={Wilson, Alex and Ker, Andrew D},
  journal={Electronic Imaging},
  volume={2016},
  number={8},
  pages={1--9},
  year={2016},
  publisher={Society for Imaging Science and Technology}
}

Honestly just read all the papers from that oxford one and all the ones that cite it
A. Wilson, P. Blunsom, and A. D. Ker, “Linguistic steganography on twitter: hierarchical language modeling with manual interaction,” in IS&T/SPIE Electronic Imaging, pp. 201–217, International Society
for Optics and Photonics, 2014.

@Comment 
@article{doc2vec,
  title={An empirical evaluation of doc2vec with practical insights into document embedding generation},
  author={Lau, Jey Han and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1607.05368},
  year={2016}
}

@inproceedings{phrases,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{paragraphs,
  title={Document embedding with paragraph vectors},
  author={Dai, Andrew M and Olah, Christopher and Le, Quoc V},
  journal={arXiv preprint arXiv:1507.07998},
  year={2015}
}

@inproceedings{seqgan,
  title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={AAAI},
  pages={2852--2858},
  year={2017}
}