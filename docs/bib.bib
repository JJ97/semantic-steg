
@Comment tyrannosaurus lex. http://web.mit.edu/keithw/tlex/ synonym based steg. Has perl implementation. Watermarking of playboy articles. Suggests WordNet to create synonym tables. Naive method suffers from one-way synonyms i.e. too -> also "the bed was too big". Only 30% of words are deemed 'useful'. Does not take context into consideration.
@inproceedings{tlex,
  title={Lexical steganography through adaptive modulation of the word choice hash},
  author={Winstein, K},
  booktitle={Secondary education at the Illinois Mathematics and Science Academy},
  year={1999},
}

@Comment uses ascii value of message to select synonym for replacement. Seeems to confuse the meaning of 'cover text'
@inproceedings{lunabel,
  title={Exploiting linguistic features in lexical steganography: design and proof-of-concept implementation},
  author={Chand, Vineeta and Orgun, C Orhan},
  booktitle={System Sciences, 2006. HICSS'06. Proceedings of the 39th Annual Hawaii International Conference on},
  volume={6},
  pages={126b--126b},
  year={2006},
  organization={IEEE}
}

@Comment Encodes using word ordering. Honestly seems a bit rubbish.
@article{chang2012secret,
  title={The secret’s in the word order: Text-to-text generation for linguistic steganography},
  author={Chang, Ching-Yun and Clark, Stephen},
  journal={Proceedings of COLING 2012},
  pages={511--528},
  year={2012}
}

@Comment uses enron email + twitter datasets. Maps bit blocks to sets of words. e.g. 00 -> {big, potato, orange} etc. Weakness in that capacity is constant throughout encoding. E.g. "the big large dog ate food" HAS to store as much data as "3-Sat is an NP-Hard decision problem". Trained their own word embeddings for some reason. Forces user to use whatever cover text it generates - often pretty rubbish. Does not use a GAN.
@article{lstm,
  title={Generating Steganographic Text with LSTMs},
  author={Fang, Tina and Jaggi, Martin and Argyraki, Katerina},
  journal={arXiv preprint arXiv:1705.10742},
  year={2017}
}

@Comment States that encoded vector is a bottleneck. Instead encoder outputs a sequence of vectors and decoder chooses which vectors to use. Potential use for lexsteg in determining words that have a large capacity? Potentially encode word to sequence of 'potential embedding vectors', then decode with message to original input + message? Need to consider gap between output word2vec and actual words. i.e. need to include gradient of vec -> closest word or training will be dodgy. Can we use the same annotation weight forumula? uses a bi-directional rnn to get hidden states in both directions. Look up beam search. 
@article{seq2seq,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{seq2seq2,
  title={Multi-task sequence to sequence learning},
  author={Luong, Minh-Thang and Le, Quoc V and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
  journal={arXiv preprint arXiv:1511.06114},
  year={2015}
}

@Comment Yelp corpus. Potential solution to our encoder-decoder gap -> just use the encoder's last hidden state. Their attempt at adding an adversarial component didn't seem to really work though...
@article{advseq2seq,
  title={Sentiment Transfer using Seq2Seq Adversarial Autoencoders},
  author={Singh, Ayush and Palod, Ritu},
  journal={arXiv preprint arXiv:1804.04003},
  year={2018}
}

@Comment Focus on image steg with JPEG. Traditional methods relying on hiding in DCT coeffs can be discovered by checking histograms. New methods of steganalysis use supervised learning. Hides data in random positions, needs error correction to compensate. Introduces idea of ε-security. Discusses high embedding strength i.e. embedding across the entire image so that cover image statistics cannot be derived vs randomised hiding.
@inproceedings{yass,
  title={YASS: Yet another steganographic scheme that resists blind steganalysis},
  author={Solanki, Kaushal and Sarkar, Anindya and Manjunath, BS},
  booktitle={International Workshop on Information Hiding},
  pages={16--31},
  year={2007},
  organization={Springer}
}

@Comment seems to just generate cover images and then apply a standard stego algo
@article{gan,
  title={Steganographic generative adversarial networks},
  author={Volkhonskiy, Denis and Nazarov, Ivan and Borisenko, Boris and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1703.05502},
  year={2017}
}


@Comment Skipgram = 1 word predicts surrounding words, CBOW = opposite
@Comment Apparently original paper was hard to reproduce. More useful to us to think of them as paragraph/sentence/tweet vectors. Finds that dbow (distributed bag of words, train document id to predict words in doc) WHY IS DBOW MORE SIMILAR TO SKIPGRAM THAN CBOW is better than dmpv (document vector + concatenated context words predicts single word). Word2Vec can be improved by using negative sampling (maximising dot prod of w against nearby words and minimising against some randomly selected non-context words) instead of softmax. skipgram -> input word, output nearby word in window.  cbow -> input is sum of words, output context word. 
@article{doc2vec,
  title={An empirical evaluation of doc2vec with practical insights into document embedding generation},
  author={Lau, Jey Han and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1607.05368},
  year={2016}
}

@Comment suggests improvements to skipgram w2v with the use of negative sampling. Also looks at identifying and learning phrases as well as words. Subsampling frequent words improves results. Hierarchical softmax uses a tree to approximate full softmax, need to evaluate log(w) output nodes instead of all w. Identify phrases simply by comparing tf of bigram vs tf of their unigrams. Code is available online!! Is it worth training my own word2vec? If there is a big enough dataset, I would argue yes.
@inproceedings{phrases,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@Comment feel like paragraph vectors may be too high level. Seem to be useful for classification of tweets but we're more concerned with changing parts of a particular tweet while conserving the meaning.
@article{paragraphs,
  title={Document embedding with paragraph vectors},
  author={Dai, Andrew M and Olah, Christopher and Le, Quoc V},
  journal={arXiv preprint arXiv:1507.07998},
  year={2015}
}

@Comment Generative RNNs suffer from exposure bias -> when training, output relies on true input but when generating it relies on previous output -> output degrades over time. Scheduled sampling helps this by feeding in output into input during training. Though Huszar shows that it doesnt work very well. Task specific loss like those used in machine translation can also help by modelling the loss on the entire sequence instead of on transitions. GANs are hard to train on discrete problems as they learn by making slight changes e.g. cant have dog+epsilon. Also hard to define loss on partially generated sequences. Seems to contradict previous point about loss on entire seq being good?? Uses monte carlo tree search on partially finished sequences to get an estimate of discriminator output at that time step. Main issue with a simple GAN for our purpose is that it would create its own cover text, autoencoder would be easier to get to transform a given cover text, i think anyway.
@inproceedings{seqgan,
  title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={AAAI},
  pages={2852--2858},
  year={2017}
}

@article{synfreq,
  title={Linguistic steganalysis using the features derived from synonym frequency},
  author={Xiang, Lingyun and Sun, Xingming and Luo, Gang and Xia, Bin},
  journal={Multimedia tools and applications},
  volume={71},
  number={3},
  pages={1893--1911},
  year={2014},
  publisher={Springer}
}

@Comment who couldve guessed that the only paper that gives real information is from Oxford? Argues that steganalysis attacks on cover generation are pointless as the generation systems can be defeated by just looking at them. Most papers have little mention of the security provided and instead look at just syntactic/semantic properties. Performed their own steganalysis and showed that individual tweet = good but a couple of samples and it becomes easy to detect. Defines 'source coding' LOOK AT J. Fridrich, Steganography in digital media: principles, algorithms, and applications. Cambridge University Press, 2009. as something that solves the 'selection channel problem' i.e. some words have better capacity. Uses PPDB for transformations. Might be useful. Really like their user interface -> user gives cover text, generates many potential stegotexts and then user selects best. Could even train network on user input to give better stegotexts over time. Capacity of stegosystem is log2 of number of paraphrases/synonyms. May want to start off by analysing capacity of our datasets with respect to word2vec synonyms. Looks at measuring distortion with respect to binary, proabilistic, edit distance and feature vectors. Dataset of 21M tweets!!!! Square root law of steganography. 
@article{covertweet,
  title={Avoiding detection on twitter: embedding strategies for linguistic steganography},
  author={Wilson, Alex and Ker, Andrew D},
  journal={Electronic Imaging},
  volume={2016},
  number={8},
  pages={1--9},
  year={2016},
  publisher={Society for Imaging Science and Technology}
}

@inproceedings{covertweet1,
  title={Linguistic steganography on twitter: hierarchical language modeling with manual interaction},
  author={Wilson, Alex and Blunsom, Phil and Ker, Andrew D},
  booktitle={Media Watermarking, Security, and Forensics 2014},
  volume={9028},
  pages={902803},
  year={2014},
  organization={International Society for Optics and Photonics}
}

@Comment concept of intentional/unintentional attacks. Trains an ngram model + SVM classifier on clean/stegged text. Attacked tlex using its lack of context awareness. 84.9% accuracy on detecting stegged text.
@inproceedings{attacks,
  title={Attacks on lexical natural language steganography systems},
  author={Taskiran, Cuneyt M and Topkara, Umut and Topkara, Mercan and Delp, Edward J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VIII},
  volume={6072},
  pages={607209},
  year={2006},
  organization={International Society for Optics and Photonics}
}

@Comment NEW REFERENCE in simmons 375 - the original paper defining the problem of steganography as two prisoners communicating through a warden. 
@Comment CHAPTER 12: STEGANOGRAPHY Main requirement is undetectibility -> no algo exists to detect if cover contains message, doesnt matter that you cant decode, detection = broken. Slightly looser constraints than imperceptibility needed for watermarking i.e. human would be able to tell there was a change but it doesnt matter since they will never see the original cover. Additionally looser constraint to watermarking in that alice can choose a different cover if it doesnt work. Sidenote: do we include spelling errors + abbreviations? would increase number of synonyms massively but probably also require custom word embeddings. Types of warden: passive -> can only prevent or permit, not modify. active -> slightly modifies all comms just in case there is a stegotext. malicious -> wardens targets this stegosystem specifically. Might be worth modelling adversarial training in these steps. I.e. yes/no discriminator, adversary changes message before decoding, says that linguistic steg is 'less developed'. Cachin analyses wardens performance with regards to hypothesis testing. Beyond this they talk a lot about steg specific to jpeg.
@book{cox,
  title={Digital watermarking and steganography},
  author={Cox, Ingemar and Miller, Matthew and Bloom, Jeffrey and Fridrich, Jessica and Kalker, Ton},
  year={2007},
  publisher={Morgan Kaufmann}
}

@article{universal,
  title={A universal lexical steganography technique},
  author={Alabish, Ahmad and Goweder, Abdulbaset and Enakoa, Anes},
  journal={International Journal of Computer and Communication Engineering},
  volume={2},
  number={2},
  pages={153},
  year={2013},
  publisher={IACSIT Press}
}


@inproceedings{autoencimagecomp,
  title={Full Resolution Image Compression with Recurrent Neural Networks.},
  author={Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  booktitle={CVPR},
  pages={5435--5443},
  year={2017}
}

@inproceedings{autoencstacking,
  title={Efficient learning of sparse representations with an energy-based model},
  author={Poultney, Christopher and Chopra, Sumit and Cun, Yann L and others},
  booktitle={Advances in neural information processing systems},
  pages={1137--1144},
  year={2007}
}

@book{goodfellow,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{seq2seqvae,
  title={Recurrent Neural Network-Based Semantic Variational Autoencoder for Sequence-to-Sequence Learning},
  author={Jang, Myeongjun and Seo, Seungwan and Kang, Pilsung},
  journal={arXiv preprint arXiv:1802.03238},
  year={2018}
}

@article{vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{transfer,
  author    = {Sandeep Subramanian and
               Adam Trischler and
               Yoshua Bengio and
               Christopher J. Pal},
  title     = {Learning General Purpose Distributed Sentence Representations via
               Large Scale Multi-task Learning},
  journal   = {CoRR},
  volume    = {abs/1804.00079},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1804.00079},
  timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{aae,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}

@article{wgan,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@article{opennmt,
  title={Opennmt: Open-source toolkit for neural machine translation},
  author={Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M},
  journal={arXiv preprint arXiv:1701.02810},
  year={2017}
}

@article{gan2,
  title={Automatic steganographic distortion learning using a generative adversarial network},
  author={Tang, Weixuan and Tan, Shunquan and Li, Bin and Huang, Jiwu},
  journal={IEEE Signal Processing Letters},
  volume={24},
  number={10},
  pages={1547--1551},
  year={2017},
  publisher={IEEE}
}

@article{suniward,
  title={Universal distortion function for steganography in an arbitrary domain},
  author={Holub, Vojt{\v{e}}ch and Fridrich, Jessica and Denemark, Tom{\'a}{\v{s}}},
  journal={EURASIP Journal on Information Security},
  volume={2014},
  number={1},
  pages={1},
  year={2014},
  publisher={Springer}
}
@article{evalall,
  title={Eval all, trust a few, do wrong to none: Comparing sentence generation models},
  author={C{\'\i}fka, Ond{\v{r}}ej and Severyn, Aliaksei and Alfonseca, Enrique and Filippova, Katja},
  journal={arXiv preprint arXiv:1804.07972},
  year={2018}
}

@article{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  journal={Text Summarization Branches Out},
  year={2004}
}

@inproceedings{outlines,
  title={Towards Text Generation with Adversarially Learned Neural Outlines},
  author={Subramanian, Sandeep and Mudumba, Sai Rajeswar and Sordoni, Alessandro and Trischler, Adam and Courville, Aaron C and Pal, Chris},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7551--7563},
  year={2018}
}

@inproceedings{bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{bleuu,
  title={Decoder integration and expected bleu training for recurrent neural network language models},
  author={Auli, Michael and Gao, Jianfeng},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  volume={2},
  pages={136--142},
  year={2014}
}

@inbook{metadata,
title = "Death by metadata: the bioinformationalisation of life and the transliteration of algorithms to flesh",
author = "Joseph Pugliese",
year = "2016",
doi = "10.1057/978-1-137-55408-6_1",
language = "English",
isbn = "9781137554079",
pages = "3--20",
editor = "Holly Randell-Moon and Ryan Tippet",
booktitle = "Security, race, biopower",
publisher = "Palgrave Macmillan",
address = "United Kingdom",
}

@inproceedings{tor,
 author = {Dingledine, Roger and Mathewson, Nick and Syverson, Paul},
 title = {Tor: The Second-generation Onion Router},
 booktitle = {Proceedings of the 13th Conference on USENIX Security Symposium - Volume 13},
 series = {SSYM'04},
 year = {2004},
 location = {San Diego, CA},
 pages = {21--21},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1251375.1251396},
 acmid = {1251396},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@article{suniward,
  title={Universal distortion function for steganography in an arbitrary domain},
  author={Holub, Vojt{\v{e}}ch and Fridrich, Jessica and Denemark, Tom{\'a}{\v{s}}},
  journal={EURASIP Journal on Information Security},
  volume={2014},
  number={1},
  pages={1},
  year={2014},
  publisher={Springer}
}

@book{watermarking,
  title={Digital watermarking and steganography: fundamentals and techniques},
  author={Shih, Frank Y},
  year={2017},
  publisher={CRC press}
}

@article{image,
  title={Digital image steganography: Survey and analysis of current methods},
  author={Cheddad, Abbas and Condell, Joan and Curran, Kevin and Mc Kevitt, Paul},
  journal={Signal processing},
  volume={90},
  number={3},
  pages={727--752},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{audio,
  title={Audio steganography using bit modification},
  author={Gopalan, Kaliappan},
  booktitle={Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on},
  volume={1},
  pages={I--629},
  year={2003},
  organization={IEEE}
}

@article{zwj,
  title={A new text steganography method by using non-printing unicode characters},
  author={Ali, Akbas E},
  journal={Engineering and Technology Journal},
  volume={28},
  number={1},
  pages={72--83},
  year={2010},
  publisher={University of Technology}
}

@inproceedings{font,
  title={Research on steganalysis for text steganography based on font format},
  author={Xiang, Lingyun and Sun, Xingming and Luo, Gang and Gan, Can},
  booktitle={Information Assurance and Security, 2007. IAS 2007. Third International Symposium on},
  pages={490--495},
  year={2007},
  organization={IEEE}
}

@article{lexsurvey,
  title={Comprehensive linguistic steganography survey},
  author={Desoky, Abdelrahman},
  journal={International Journal of Information and Computer Security},
  volume={4},
  number={2},
  pages={164--197},
  year={2010},
  publisher={Inderscience Publishers}
}

@article{wordnet,
 author = {Miller, George A.},
 title = {WordNet: A Lexical Database for English},
 journal = {Commun. ACM},
 issue_date = {Nov. 1995},
 volume = {38},
 number = {11},
 month = nov,
 year = {1995},
 issn = {0001-0782},
 pages = {39--41},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/219717.219748},
 doi = {10.1145/219717.219748},
 acmid = {219748},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{syn1,
  title={Synonymous paraphrasing using wordnet and internet},
  author={Bolshakov, Igor A and Gelbukh, Alexander},
  booktitle={International Conference on Application of Natural Language to Information Systems},
  pages={312--323},
  year={2004},
  organization={Springer}
}

@inproceedings{syn2,
 author = {Topkara, Umut and Topkara, Mercan and Atallah, Mikhail J.},
 title = {The Hiding Virtues of Ambiguity: Quantifiably Resilient Watermarking of Natural Language Text Through Synonym Substitutions},
 booktitle = {Proceedings of the 8th Workshop on Multimedia and Security},
 series = {MM\&\#38;Sec '06},
 year = {2006},
 isbn = {1-59593-493-6},
 location = {Geneva, Switzerland},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1161366.1161397},
 doi = {10.1145/1161366.1161397},
 acmid = {1161397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {homograph, information hiding, natural language text, synonym substitution},
} 

@inproceedings{parsetree,
  title={Words are not enough: sentence level natural language watermarking},
  author={Topkara, Mercan and Topkara, Umut and Atallah, Mikhail J},
  booktitle={Proceedings of the 4th ACM international workshop on Contents protection and security},
  pages={37--46},
  year={2006},
  organization={ACM}
}

@article{cont,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  journal={arXiv preprint arXiv:1511.06349},
  year={2015}
}

@article{ngram,
  title={Practical linguistic steganography using contextual synonym substitution and a novel vertex coding method},
  author={Chang, Ching-Yun and Clark, Stephen},
  journal={Computational Linguistics},
  volume={40},
  number={2},
  pages={403--448},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{bib,
  title={A comprehensive bibliography of linguistic steganography},
  author={Bergmair, Richard},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents IX},
  volume={6505},
  pages={65050W},
  year={2007},
  organization={International Society for Optics and Photonics}
}
@article{nicetext,
  title={Nicetext},
  author={Chapman, MT and Davida, GI},
  journal={Website, August},
  year={1997}
}

@inproceedings{nicetext2,
  title={A practical and effective approach to large-scale automated linguistic steganography},
  author={Chapman, Mark and Davida, George I and Rennhard, Marc},
  booktitle={International Conference on Information Security},
  pages={156--165},
  year={2001},
  organization={Springer}
}

@inproceedings{presup,
  title={A method of text watermarking using presuppositions},
  author={Vybornova, Olga and Macq, Benoit},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents IX},
  volume={6505},
  pages={65051R},
  year={2007},
  organization={International Society for Optics and Photonics}
}

@inproceedings{errors,
  title={Information hiding through errors: a confusing approach},
  author={Topkara, Mercan and Topkara, Umut and Atallah, Mikhail J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents IX},
  volume={6505},
  pages={65050V},
  year={2007},
  organization={International Society for Optics and Photonics}
}

@inproceedings{ppdb,
  title={PPDB: The paraphrase database},
  author={Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={758--764},
  year={2013}
}

@inproceedings{trans,
  title={Translation-based steganography},
  author={Grothoff, Christian and Grothoff, Krista and Alkhutova, Ludmila and Stutsman, Ryan and Atallah, Mikhail},
  booktitle={International Workshop on Information Hiding},
  pages={219--233},
  year={2005},
  organization={Springer}
}

@article{primer,
  title={A primer on neural network models for natural language processing},
  author={Goldberg, Yoav},
  journal={Journal of Artificial Intelligence Research},
  volume={57},
  pages={345--420},
  year={2016}
}


@article{queen,
  title={Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning},
  author={Vylomova, Ekaterina and Rimell, Laura and Cohn, Trevor and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1509.01692},
  year={2015}
}

@article{word2vec,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{elmo,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@inproceedings{skipthought,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in neural information processing systems},
  pages={3294--3302},
  year={2015}
}

@article{universal,
  title={Universal sentence encoder},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  journal={arXiv preprint arXiv:1803.11175},
  year={2018}
}

@inproceedings{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{transdec,
  title={Generating wikipedia by summarizing long sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  journal={arXiv preprint arXiv:1801.10198},
  year={2018}
}

@article{paranmt,
  title={Paranmt-50m: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations},
  author={Wieting, John and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1711.05732},
  year={2017}
}
