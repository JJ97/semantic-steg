
@Comment tyrannosaurus lex. http://web.mit.edu/keithw/tlex/ synonym based steg. Has perl implementation. Watermarking of playboy articles. Suggests WordNet to create synonym tables. Naive method suffers from one-way synonyms i.e. too -> also "the bed was too big". Only 30% of words are deemed 'useful'. Does not take context into consideration.
@inproceedings{tlex,
  title={Lexical steganography through adaptive modulation of the word choice hash},
  author={Winstein, K},
  booktitle={Secondary education at the Illinois Mathematics and Science Academy},
  year={1999},
}

@Comment uses ascii value of message to select synonym for replacement. Seeems to confuse the meaning of 'cover text'
@inproceedings{lunabel,
  title={Exploiting linguistic features in lexical steganography: design and proof-of-concept implementation},
  author={Chand, Vineeta and Orgun, C Orhan},
  booktitle={System Sciences, 2006. HICSS'06. Proceedings of the 39th Annual Hawaii International Conference on},
  volume={6},
  pages={126b--126b},
  year={2006},
  organization={IEEE}
}

@Comment Encodes using word ordering. Honestly seems a bit rubbish.
@article{chang2012secret,
  title={The secret’s in the word order: Text-to-text generation for linguistic steganography},
  author={Chang, Ching-Yun and Clark, Stephen},
  journal={Proceedings of COLING 2012},
  pages={511--528},
  year={2012}
}

@Comment uses enron email + twitter datasets. Maps bit blocks to sets of words. e.g. 00 -> {big, potato, orange} etc. Weakness in that capacity is constant throughout encoding. E.g. "the big large dog ate food" HAS to store as much data as "3-Sat is an NP-Hard decision problem". Trained their own word embeddings for some reason. Forces user to use whatever cover text it generates - often pretty rubbish. Does not use a GAN.
@article{lstm,
  title={Generating Steganographic Text with LSTMs},
  author={Fang, Tina and Jaggi, Martin and Argyraki, Katerina},
  journal={arXiv preprint arXiv:1705.10742},
  year={2017}
}

@Comment States that encoded vector is a bottleneck. Instead encoder outputs a sequence of vectors and decoder chooses which vectors to use. Potential use for lexsteg in determining words that have a large capacity? Potentially encode word to sequence of 'potential embedding vectors', then decode with message to original input + message? Need to consider gap between output word2vec and actual words. i.e. need to include gradient of vec -> closest word or training will be dodgy. Can we use the same annotation weight forumula? uses a bi-directional rnn to get hidden states in both directions. Look up beam search. 
@article{seq2seq,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{seq2seq2,
  title={Multi-task sequence to sequence learning},
  author={Luong, Minh-Thang and Le, Quoc V and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
  journal={arXiv preprint arXiv:1511.06114},
  year={2015}
}

@Comment Yelp corpus. Really interesting idea but not sure why they give the discriminator the hidden state instead of the output, english is quite poor and only discussion of accuracy instead of f1
@article{advseq2seq,
  title={Sentiment Transfer using Seq2Seq Adversarial Autoencoders},
  author={Singh, Ayush and Palod, Ritu},
  journal={arXiv preprint arXiv:1804.04003},
  year={2018}
}

@Comment Focus on image steg with JPEG. Traditional methods relying on hiding in DCT coeffs can be discovered by checking histograms. New methods of steganalysis use supervised learning. Hides data in random positions, needs error correction to compensate. Introduces idea of ε-security. Discusses high embedding strength i.e. embedding across the entire image so that cover image statistics cannot be derived vs randomised hiding.
@inproceedings{yass,
  title={YASS: Yet another steganographic scheme that resists blind steganalysis},
  author={Solanki, Kaushal and Sarkar, Anindya and Manjunath, BS},
  booktitle={International Workshop on Information Hiding},
  pages={16--31},
  year={2007},
  organization={Springer}
}

@Comment seems to just generate cover images and then apply a standard stego algo
@article{gan,
  title={Steganographic generative adversarial networks},
  author={Volkhonskiy, Denis and Nazarov, Ivan and Borisenko, Boris and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1703.05502},
  year={2017}
}


@Comment Skipgram = 1 word predicts surrounding words, CBOW = opposite
@Comment Apparently original paper was hard to reproduce. More useful to us to think of them as paragraph/sentence/tweet vectors. Finds that dbow (distributed bag of words, train document id to predict words in doc) WHY IS DBOW MORE SIMILAR TO SKIPGRAM THAN CBOW is better than dmpv (document vector + concatenated context words predicts single word). Word2Vec can be improved by using negative sampling (maximising dot prod of w against nearby words and minimising against some randomly selected non-context words) instead of softmax. skipgram -> input word, output nearby word in window.  cbow -> input is sum of words, output context word. 
@article{doc2vec,
  title={An empirical evaluation of doc2vec with practical insights into document embedding generation},
  author={Lau, Jey Han and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1607.05368},
  year={2016}
}

@Comment suggests improvements to skipgram w2v with the use of negative sampling. Also looks at identifying and learning phrases as well as words. Subsampling frequent words improves results. Hierarchical softmax uses a tree to approximate full softmax, need to evaluate log(w) output nodes instead of all w. Identify phrases simply by comparing tf of bigram vs tf of their unigrams. Code is available online!! Is it worth training my own word2vec? If there is a big enough dataset, I would argue yes.
@inproceedings{phrases,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@Comment feel like paragraph vectors may be too high level. Seem to be useful for classification of tweets but we're more concerned with changing parts of a particular tweet while conserving the meaning.
@article{paragraphs,
  title={Document embedding with paragraph vectors},
  author={Dai, Andrew M and Olah, Christopher and Le, Quoc V},
  journal={arXiv preprint arXiv:1507.07998},
  year={2015}
}

@Comment Generative RNNs suffer from exposure bias -> when training, output relies on true input but when generating it relies on previous output -> output degrades over time. Scheduled sampling helps this by feeding in output into input during training. Though Huszar shows that it doesnt work very well. Task specific loss like those used in machine translation can also help by modelling the loss on the entire sequence instead of on transitions. GANs are hard to train on discrete problems as they learn by making slight changes e.g. cant have dog+epsilon. Also hard to define loss on partially generated sequences. Seems to contradict previous point about loss on entire seq being good?? Uses monte carlo tree search on partially finished sequences to get an estimate of discriminator output at that time step. Main issue with a simple GAN for our purpose is that it would create its own cover text, autoencoder would be easier to get to transform a given cover text, i think anyway.
@inproceedings{seqgan,
  title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={AAAI},
  pages={2852--2858},
  year={2017}
}

@article{synfreq,
  title={Linguistic steganalysis using the features derived from synonym frequency},
  author={Xiang, Lingyun and Sun, Xingming and Luo, Gang and Xia, Bin},
  journal={Multimedia tools and applications},
  volume={71},
  number={3},
  pages={1893--1911},
  year={2014},
  publisher={Springer}
}

@Comment who couldve guessed that the only paper that gives real information is from Oxford? Argues that steganalysis attacks on cover generation are pointless as the generation systems can be defeated by just looking at them. Most papers have little mention of the security provided and instead look at just syntactic/semantic properties. Performed their own steganalysis and showed that individual tweet = good but a couple of samples and it becomes easy to detect. Defines 'source coding' LOOK AT J. Fridrich, Steganography in digital media: principles, algorithms, and applications. Cambridge University Press, 2009. as something that solves the 'selection channel problem' i.e. some words have better capacity. Uses PPDB for transformations. Might be useful. Really like their user interface -> user gives cover text, generates many potential stegotexts and then user selects best. Could even train network on user input to give better stegotexts over time. Capacity of stegosystem is log2 of number of paraphrases/synonyms. May want to start off by analysing capacity of our datasets with respect to word2vec synonyms. Looks at measuring distortion with respect to binary, proabilistic, edit distance and feature vectors. Dataset of 21M tweets!!!! Square root law of steganography. 
@article{covertweet,
  title={Avoiding detection on twitter: embedding strategies for linguistic steganography},
  author={Wilson, Alex and Ker, Andrew D},
  journal={Electronic Imaging},
  volume={2016},
  number={8},
  pages={1--9},
  year={2016},
  publisher={Society for Imaging Science and Technology}
}

@inproceedings{covertweet1,
  title={Linguistic steganography on twitter: hierarchical language modeling with manual interaction},
  author={Wilson, Alex and Blunsom, Phil and Ker, Andrew D},
  booktitle={Media Watermarking, Security, and Forensics 2014},
  volume={9028},
  pages={902803},
  year={2014},
  organization={International Society for Optics and Photonics}
}

@Comment concept of intentional/unintentional attacks. Trains an ngram model + SVM classifier on clean/stegged text. Attacked tlex using its lack of context awareness. 84.9% accuracy on detecting stegged text.
@inproceedings{attacks,
  title={Attacks on lexical natural language steganography systems},
  author={Taskiran, Cuneyt M and Topkara, Umut and Topkara, Mercan and Delp, Edward J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VIII},
  volume={6072},
  pages={607209},
  year={2006},
  organization={International Society for Optics and Photonics}
}

@Comment NEW REFERENCE in simmons 375 - the original paper defining the problem of steganography as two prisoners communicating through a warden. 
@Comment CHAPTER 12: STEGANOGRAPHY Main requirement is undetectibility -> no algo exists to detect if cover contains message, doesnt matter that you cant decode, detection = broken. Slightly looser constraints than imperceptibility needed for watermarking i.e. human would be able to tell there was a change but it doesnt matter since they will never see the original cover. Additionally looser constraint to watermarking in that alice can choose a different cover if it doesnt work. Sidenote: do we include spelling errors + abbreviations? would increase number of synonyms massively but probably also require custom word embeddings. Types of warden: passive -> can only prevent or permit, not modify. active -> slightly modifies all comms just in case there is a stegotext. malicious -> wardens targets this stegosystem specifically. Might be worth modelling adversarial training in these steps. I.e. yes/no discriminator, adversary changes message before decoding, says that linguistic steg is 'less developed'. Cachin analyses wardens performance with regards to hypothesis testing. Beyond this they talk a lot about steg specific to jpeg.
@book{cox,
  title={Digital watermarking and steganography},
  author={Cox, Ingemar and Miller, Matthew and Bloom, Jeffrey and Fridrich, Jessica and Kalker, Ton},
  year={2007},
  publisher={Morgan Kaufmann}
}

@article{universal,
  title={A universal lexical steganography technique},
  author={Alabish, Ahmad and Goweder, Abdulbaset and Enakoa, Anes},
  journal={International Journal of Computer and Communication Engineering},
  volume={2},
  number={2},
  pages={153},
  year={2013},
  publisher={IACSIT Press}
}